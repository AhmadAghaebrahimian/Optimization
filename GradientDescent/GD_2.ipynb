{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNiNeJcy5mR+TW1XJLORNcM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AhmadAghaebrahimian/Optimization/blob/main/GradientDescent/GD_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this excercise, we implement Gradient Decent for a function with one multidimensioanl variable. We want to minimize the function (hypothesis) $h(x) = \\theta x + b$ where $x$, $\\theta$ are multidimensioanl vectors. We also use the Sigmoid function (or otherwise known as the logistic function, hense the method logistic regression)\n",
        "\n",
        "$\\frac{1}{1+e^{-f(x)}}$\n",
        "\n",
        "to transform the output of $h(x)$ from real value to an integr value. This is the setting for a classification problem using logistic regression where the hypothesis maps an n-dimensioanl feature space to an integer (id of a class).\n",
        "\n",
        "Let's import dependencies and get started."
      ],
      "metadata": {
        "id": "m0YgVo4Cw7AR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "vhshBUHmhDet"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "\n",
        "#suppress warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use the Tumor dataset for training the model. It consists of 30 different measurements (feature) of 569 moles (instances). Each instance is assigned to one label, either Malignant or Benign as the type of the tumor.\n",
        "\n",
        "Let's load the dataset."
      ],
      "metadata": {
        "id": "6b8jfnteo2vQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "link = 'https://drive.google.com/uc?export=download&id=1m1s6Q7xQfdWf642OqMkzu2nmvoJ0xK_5'\n",
        "data = pd.read_csv(link)\n",
        "# data = data.sample(frac=1).reset_index(drop=True)\n",
        "d = {'M': 1, 'B': 0} # we map labels from string to integer\n",
        "data['diagnosis'] = data['diagnosis'].map(d)\n",
        "print(data.head())"
      ],
      "metadata": {
        "id": "jJ2DUvUIo260"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Although it is not directly related to optimization, a good practise in Machine Learning is to have three seperate data each for paticular purpose; training, development, and test set. Training data is used to tune the model parameters $\\theta$ while development data is used to tune hyperparamerts related to overall algorthms such as learning rate. Test data are reserved for when data scietist are satisfied with results and want to report a final results.\n",
        "\n",
        "Let's seperate the Tumor data into standard datasets; 80% for training, 10% for development, and 10% for testing."
      ],
      "metadata": {
        "id": "F08AkRIH9MBF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_idx = int(data.shape[0]*0.9)\n",
        "val_idx = int(data.shape[0]*0.8)\n",
        "\n",
        "test_data = data[test_idx:]\n",
        "val_data = data[val_idx:test_idx]\n",
        "data = data[:val_idx]\n",
        "\n",
        "train_Y, train_X = data['diagnosis'], data.drop('diagnosis', axis=1)\n",
        "val_Y, val_X = val_data['diagnosis'], val_data.drop('diagnosis', axis=1)\n",
        "test_Y, test_X = test_data['diagnosis'], test_data.drop('diagnosis', axis=1)\n",
        "\n",
        "print('Training data shape: ', train_X.shape)\n",
        "print('Validation data shape: ', val_X.shape)\n",
        "print('Testing data shape: ', test_X.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51X5jUopqSpE",
        "outputId": "03465dc2-68c3-4965-b5b4-57f5c0c61aa1"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data shape:  (455, 30)\n",
            "Validation data shape:  (57, 30)\n",
            "Testing data shape:  (57, 30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we implement the Sigmoid function.\n",
        "\n",
        "$s(x) = \\frac{1}{1+e^{-f(x)}}$\n",
        "\n",
        "It squeezes the real-value output of $h(x)$ to a probabiliy distribution that can be trnasformed to $1$, or $0$, by defining a threshold like $0.5$:"
      ],
      "metadata": {
        "id": "TK3X0hY4qVEs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(x):\n",
        "    return 1/(1+np.exp(-x))"
      ],
      "metadata": {
        "id": "08C0Vnk-qah7"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's workout the gradient step:\n",
        "\n",
        "Hers is the hypothesis. For the sake of simplicity we ignored the bias term. As a practce add a bias terms and make the required changes. $h(x) = \\theta^\\top x$\n",
        "\n",
        "Similar to Excecise 1, we could use MSE for computing the loss.\n",
        "\n",
        "$\\ell (\\theta) = \\frac{1}{m} \\sum_{i=1}^m {(\\hat y - y)}^2$\n",
        "\n",
        "However, since $\\hat y$ is not linear in logistic regression, $\\ell$ above is not convex, thus not suitable for optimization. To spare from some complexities, let's ignore the derivation and directly define and implement the update rule as the following:\n",
        "\n",
        "$\\theta_{j\\_new} := \\theta_{j\\_old} - \\alpha x^i_j(y^i - s(x^i))$\n"
      ],
      "metadata": {
        "id": "5RHFXNkiqcr-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_step(theta, X, y, lr):\n",
        "    h_x = X.dot(theta)\n",
        "    y_hat = np.array(sigmoid(h_x))\n",
        "    theta -= lr * (X.T.dot(y_hat - y))\n",
        "    return theta\n",
        "\n",
        "def gradient_descent(lr, n_iterations):\n",
        "    theta = np.ones((30, 1))\n",
        "\n",
        "    iter_count = 0\n",
        "    val_accuracies = []\n",
        "    while iter_count < n_iterations:\n",
        "        val_accuracy = get_accuracy(val_X, val_Y.to_frame(), theta)\n",
        "        print('Epoch {}: Validation Accuracy {}%'.format(iter_count, val_accuracy))\n",
        "        val_accuracies.append(val_accuracy)\n",
        "\n",
        "        for i in range(0, train_X.shape[0], batch_size):\n",
        "            theta = gradient_step(theta, train_X[i:i+batch_size], train_Y[i:i+batch_size].to_frame(), lr)\n",
        "        iter_count += 1\n",
        "\n",
        "    return iter_count, theta, val_accuracies"
      ],
      "metadata": {
        "id": "rZAPq6h8qhba"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before running the optimization, we define a metric to evaluate how well the parameters are optimized. For this purpose, we use the Accuracy as the ratio of the number of correctly classified cases against all cases.\n",
        "\n",
        "Let's implement a function to compute accuracy:"
      ],
      "metadata": {
        "id": "3ueshiqtqpL8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_accuracy(X, y, theta):\n",
        "    y_hat = np.array(sigmoid(X.dot(theta)))\n",
        "\n",
        "    ## Converting y_hat probabilities to prediction, >.5 = 1(M), <.5 = 0(B)\n",
        "    predictions = np.greater(y_hat, 0.5 * np.ones((y_hat.shape[1], 1)))\n",
        "    accuracy = np.count_nonzero(np.equal(predictions, y)) / predictions.shape[0] * 100\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "8u6LPUB8qqTc"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameters"
      ],
      "metadata": {
        "id": "dAWEWlSCqs0l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 50\n",
        "lr = 0.003\n",
        "n_iterations = 100"
      ],
      "metadata": {
        "id": "SSCXV4WTqv4H"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, everthing is in place. Let's run everything together and visualize the accuracy curve."
      ],
      "metadata": {
        "id": "7u3ZUa34qzVi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "iter_count, theta, val_accuracies = gradient_descent(lr, n_iterations)\n",
        "print('After {} Iterations'.format(iter_count))\n",
        "print('Test Accuracy: {}%'.format(get_accuracy(test_X, test_Y.to_frame(), theta)))\n",
        "\n",
        "plt.plot(range(len(val_accuracies)), val_accuracies)\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KCBZ7Q8pqyKO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}