{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMFsSX36OsN3BdACdFt823u",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AhmadAghaebrahimian/Optimization/blob/main/GradientDescent/GD_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this exercise, we implement Gradient Decent for a function with one multidimensional variable. We want to minimize the function (hypothesis) $h(x) = \\theta x + b$ where $x$ and $\\theta$ are multidimensioanl vectors. We also use the Sigmoid function (otherwise known as the logistic function, hence the method of Logistic Regression)\n",
        "\n",
        "$\\frac{1}{1+e^{-f(x)}}$\n",
        "\n",
        "to transform the output of $h(x)$ from a real value to a probability distribution and then to an integer value. This is the setting for a classification problem using logistic regression where the hypothesis maps an n-dimensional feature space to an integer (id of a class).\n",
        "\n",
        "Let's import dependencies and get started."
      ],
      "metadata": {
        "id": "m0YgVo4Cw7AR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vhshBUHmhDet"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "\n",
        "#suppress warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use the Tumor dataset for training the model. It consists of 30 different measurements (features) of 569 moles (instances). Each instance is assigned to one label, either Malignant or Benign as the type of the tumor.\n",
        "\n",
        "Let's load the dataset."
      ],
      "metadata": {
        "id": "6b8jfnteo2vQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "link = 'https://drive.google.com/uc?export=download&id=1m1s6Q7xQfdWf642OqMkzu2nmvoJ0xK_5'\n",
        "data = pd.read_csv(link)\n",
        "# data = data.sample(frac=1).reset_index(drop=True)\n",
        "d = {'M': 1, 'B': 0} # we map labels from string to integer\n",
        "data['diagnosis'] = data['diagnosis'].map(d)\n",
        "print(data.head())"
      ],
      "metadata": {
        "id": "jJ2DUvUIo260"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Although it is not directly related to optimization, a good practice in Machine Learning is to have three separate data each for a particular purpose; training, development, and test set. Training data is used to tune the model parameters $\\theta$ while development data is used to tune hyperparameters related to overall algorithms such as the learning rate. Test data are reserved for when data scientists are satisfied with the results and want to report the final results.\n",
        "\n",
        "Let's separate the Tumor data into standard datasets; 80% for training, 10% for development, and 10% for testing."
      ],
      "metadata": {
        "id": "F08AkRIH9MBF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_idx = int(data.shape[0]*0.9)\n",
        "val_idx = int(data.shape[0]*0.8)\n",
        "\n",
        "test_data = data[test_idx:]\n",
        "val_data = data[val_idx:test_idx]\n",
        "data = data[:val_idx]\n",
        "\n",
        "train_Y, train_X = data['diagnosis'], data.drop('diagnosis', axis=1)\n",
        "val_Y, val_X = val_data['diagnosis'], val_data.drop('diagnosis', axis=1)\n",
        "test_Y, test_X = test_data['diagnosis'], test_data.drop('diagnosis', axis=1)\n",
        "\n",
        "print('Training data shape: ', train_X.shape)\n",
        "print('Validation data shape: ', val_X.shape)\n",
        "print('Testing data shape: ', test_X.shape)"
      ],
      "metadata": {
        "id": "51X5jUopqSpE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we implement the Sigmoid function.\n",
        "\n",
        "$s(x) = \\frac{1}{1+e^{-h(x)}}$\n",
        "\n",
        "It squeezes the real-value output of $h(x)$ to a probabiliy distribution that can be trnasformed to $1$, or $0$, by defining a threshold like $0.5$:"
      ],
      "metadata": {
        "id": "TK3X0hY4qVEs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(x):\n",
        "    return 1/(1+np.exp(-x))"
      ],
      "metadata": {
        "id": "08C0Vnk-qah7"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also need to define a metric to evaluate how well the parameters are optimized. For this purpose, we use the Accuracy as the ratio of the number of correctly classified cases against all cases.\n",
        "\n",
        "Let's implement a function to compute accuracy:"
      ],
      "metadata": {
        "id": "3ueshiqtqpL8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_accuracy(X, y, theta):\n",
        "    y_hat = np.array(sigmoid(X.dot(theta)))\n",
        "\n",
        "    ## Converting y_hat probabilities to prediction, >.5 = 1(M), <.5 = 0(B)\n",
        "    predictions = np.greater(y_hat, 0.5 * np.ones((y_hat.shape[1], 1)))\n",
        "    accuracy = np.count_nonzero(np.equal(predictions, y)) / predictions.shape[0] * 100\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "8u6LPUB8qqTc"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's work on the gradient step:\n",
        "\n",
        "Hers is the hypothesis. For the sake of simplicity, we ignored the bias term.\n",
        "\n",
        "$\\hat y = \\frac{1}{1+e^{-\\theta^\\top x}}$\n",
        "\n",
        "Similar to exercise 1, we could use MSE for computing the loss.\n",
        "\n",
        "$\\ell (\\theta) = \\frac{1}{2m} \\sum_{i=1}^m {(\\hat y - y)}^2$\n",
        "\n",
        "However, since $\\hat y$ is not linear in logistic regression, $\\ell$ above is not convex, thus not suitable for optimization. To spare some complexities, let's ignore the derivation and directly define and implement the update rule as follows:\n",
        "\n",
        "$\\theta_{j\\_new} := \\theta_{j\\_old} - \\alpha x^i_j(\\hat y^i - y^i)$\n"
      ],
      "metadata": {
        "id": "5RHFXNkiqcr-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_step(theta, X, y, lr):\n",
        "    y_hat = np.array(sigmoid(X.dot(theta)))\n",
        "    theta -= lr * (X.T.dot(y_hat - y))\n",
        "    return theta\n",
        "\n",
        "def gradient_descent(lr, n_iterations):\n",
        "    theta = np.ones((30, 1)) # initial theta\n",
        "\n",
        "    iter_count = 0\n",
        "    val_accuracies = []\n",
        "    while iter_count < n_iterations:\n",
        "        val_accuracy = get_accuracy(val_X, val_Y.to_frame(), theta)\n",
        "        print('Epoch {}: Validation Accuracy {}%'.format(iter_count, val_accuracy))\n",
        "        val_accuracies.append(val_accuracy)\n",
        "\n",
        "        for i in range(0, train_X.shape[0], batch_size):\n",
        "            theta = gradient_step(theta, train_X[i:i+batch_size], train_Y[i:i+batch_size].to_frame(), lr)\n",
        "        iter_count += 1\n",
        "\n",
        "    return iter_count, theta, val_accuracies"
      ],
      "metadata": {
        "id": "rZAPq6h8qhba"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameters"
      ],
      "metadata": {
        "id": "dAWEWlSCqs0l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 50\n",
        "lr = 0.003\n",
        "n_iterations = 50"
      ],
      "metadata": {
        "id": "SSCXV4WTqv4H"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, everything is in place. Let's run everything together and visualize the accuracy curve."
      ],
      "metadata": {
        "id": "7u3ZUa34qzVi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "iter_count, theta, val_accuracies = gradient_descent(lr, n_iterations)\n",
        "print('After {} Iterations'.format(iter_count))\n",
        "print('Test Accuracy: {}%'.format(get_accuracy(test_X, test_Y.to_frame(), theta)))\n",
        "\n",
        "plt.plot(range(len(val_accuracies)), val_accuracies)\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KCBZ7Q8pqyKO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optional Exercise 1: Add a bias term to $h(x)$ and make the required changes.\n",
        "\n",
        "Optional Exercise 2: Scale the features and run again. Notice any difference?\n",
        "\n",
        "Optional Exercise 3: Assess the impact of too small/big learning rate along a number of iteration\n",
        "\n",
        "Optional Exercise 4: Assess the impact of other types of initialization (all ones, all zeros, random numbers with different mean and standard deviations) on the learning curve."
      ],
      "metadata": {
        "id": "BrTuiOMjerp2"
      }
    }
  ]
}