{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO1WEn/t3IwrM9c+PPDnWPa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AhmadAghaebrahimian/Optimization/blob/main/GradientDescent/GD_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this excercise, we implement Gradient Decent for a simple one variable function. Let's assume that we want to minimize the function $f(x) = \\theta x + b$. This is a simple linear model where $Θ$ is the slope and $b$ is the y-intercept. To optimize (minimize/mazimize) $f(x)$, we need to find optimum $Θ$ and $b$ which from now on are called the parameters and the bias respectively. Since $x$ can be multidimentional, $Θ$ can also be multidimesional, i.e., one parameter for each dimension of $x$. For the sake of simplicity though, we assume $x$ hence $\\theta$ are one dimention in this excercise. We also ignore bias for now. In the next excercise, we examine Gradient Descent for multidimensional optimization. Therefore, we optimize $f(x) = \\theta x$.\n",
        "\n",
        "Let's import dependencies and get started.\n",
        "\n"
      ],
      "metadata": {
        "id": "iPUAPZzO9aln"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ALt6nL8ak3S_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need some training data which consists of a set of tuples $(x_i, y_i)$.\n",
        "\n",
        "Let's generate some hypothetical training data.\n"
      ],
      "metadata": {
        "id": "sQy924KyEEfk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(42) # Ensure replicability; Randomely generated numbers stay the same in different runs\n",
        "X = 2 * np.random.rand(10000, 1)  # one dimensional Xs\n",
        "y = 3 * X + np.random.randn(10000, 1)  # Assumng we know the correct parameter so that we can generate ground truths plus a little bit of noise\n",
        "print (\"First instance of training data: (%d, %d)\"%(X[0], y[0]))"
      ],
      "metadata": {
        "id": "-vJMG7ZBk9Eh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5127bb2f-d44b-4a38-98a4-1cf43e74015d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First instance of training data: (0, 0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "$x_i$ is the independent and $y_i$ is the dependent variables. $y_i$ is ground truth while $\\hat y$ is the predicted value by the model (or hypothesis) $(\\hat y = h(x) = \\theta \\hat x)$. We will find $\\theta$ so that the sum of all errors $(\\sum (\\hat y - y))$ becoma as small as possible. This is called loss (or objective) function. To make it more mathematically elegant, let's square the error and average it all (hense the well-know loss function; Mean of Square Erros, MSE).\n",
        "\n",
        "$\\ell (\\theta) = \\frac{1}{m} \\sum_{i=1}^m {(\\hat y - y)}^2$\n",
        "\n",
        "Gradient Descent uses the derivative of $\\ell (\\theta)$ with respect to each $\\theta$ (i.e. gradients) to find the direction of the next move toward the optima. Here it is the Gradient Descent update rule:\n",
        "\n",
        "$\\theta_{new} := \\theta_{old} - \\alpha \\nabla f(\\theta)$ which is the same as:\n",
        "\n",
        "$\\theta_{j\\_new} := \\theta_{j\\_old} - \\alpha x^i_j (\\hat{y}^i - y^i)$ which is the same as:\n",
        "\n",
        "$\\theta_{j\\_new} := \\theta_{j\\_old} - \\alpha x^i_j(\\theta \\hat x - y^i)$\n",
        "\n",
        "Knowing all of this, let's implement the gradient descent function:\n"
      ],
      "metadata": {
        "id": "svQtJIFNFFtK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent(X, y, learning_rate, n_iterations):\n",
        "    m = len(y)\n",
        "    theta = np.random.randn(1, 1)  # Initially assign random values to theta\n",
        "    loss_history = [] # save the loss history to visualize learning curve\n",
        "\n",
        "    for iteration in range(n_iterations):\n",
        "        gradients = 1/m * X.T.dot(X.dot(theta) - y)\n",
        "        theta -= learning_rate * gradients # parameters updated\n",
        "        loss = np.mean((X.dot(theta) - y)**2) # compute loss with new parameters\n",
        "        loss_history.append(loss)\n",
        "\n",
        "    return theta, loss_history"
      ],
      "metadata": {
        "id": "xHCppNQXk_Bf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Gradient step is ready. It requires two hyperparameters to run. In contrast to parameters which are tuned on training data, hyperparamerts are tuned on validation data to fine-tune the optimization procedure. Here we have two hyperparameters; learning rate and number of iteration. Leraning rate determines how big the jump toward the optima should be after the gradient recognizes the direction of the jump. Too big jumps make too large strides which lead to divergence. Too small jump makes it too long to reach the optima.\n",
        "\n",
        "The second hyper parameter is the number of iteration, which dictates how many time the entire training data should be passed to Gradient Descent until it converges.\n",
        "\n",
        "Convergence is the point the loss does not reduces anymore which signals the model does not learn anymore. It can be checked by computing the different between previous and current loss in each iteratin.\n",
        "\n",
        "Let's implement these concepts before running the optimization step:"
      ],
      "metadata": {
        "id": "tmYEA8yFPKEs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.1\n",
        "n_iterations = 50"
      ],
      "metadata": {
        "id": "JpPCF3rglBsT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now everything is in place. Let's run the code and see how it works by visualzing its loss curve."
      ],
      "metadata": {
        "id": "foC8rg4RRlcY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "theta_best, loss_history = gradient_descent(X, y, learning_rate, n_iterations)\n",
        "\n",
        "plt.plot(range(n_iterations), loss_history)\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Cost')\n",
        "plt.title('Gradient Descent Convergence')\n",
        "plt.show()\n",
        "#\n",
        "print(\"Theta found by Gradient Descent:\", theta_best.ravel())"
      ],
      "metadata": {
        "id": "FbIL5o1-Rdqr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0jl4u23qlIXi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}