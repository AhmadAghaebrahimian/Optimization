{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM8cISeJXBPiNp3Z96nHChc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AhmadAghaebrahimian/Optimization/blob/main/GradientDescent/GD_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this excercise, we implement Gradient Decent for a simple one variable function. Let's assume that we want to minimize the function $f(x) = \\theta x + b$. This is a simple linear model where $\\theta$ is the slope and $b$ is the y-intercept. To optimize (minimize/mazimize) $f(x)$, we need to find optimum $\\theta$ and $b$ which from now on are called the parameters and the bias respectively. Since $x$ can be multidimentional, $\\theta$ can also be multidimesional, i.e., one parameter for each dimension of $x$. For the sake of simplicity though, we assume $x$ hence $\\theta$ are one dimention in this excercise. We also ignore bias for now. In the next excercise, we examine Gradient Descent for multidimensional optimization. So, let'se optimize $f(x) = \\theta x$.\n",
        "\n",
        "Let's import dependencies first.\n",
        "\n"
      ],
      "metadata": {
        "id": "iPUAPZzO9aln"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ALt6nL8ak3S_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need some training data which consists of a set of tuples $(x_i, y_i)$.\n",
        "\n",
        "Let's generate some hypothetical training data.\n"
      ],
      "metadata": {
        "id": "sQy924KyEEfk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(31) # Ensure replicability; Randomely generated numbers stay the same in different runs\n",
        "X = 2 * np.random.rand(10000, 1)  # one dimensional Xs\n",
        "y = 3 * X + np.random.randn(10000, 1)  # Assumng we know the correct parameter so that we can generate ground truths plus a little bit of noise\n",
        "print (\"First instance of training data: (%d, %d)\"%(X[0], y[0]))"
      ],
      "metadata": {
        "id": "-vJMG7ZBk9Eh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "$x_i$ is the independent and $y_i$ is the dependent variables. $y_i$ is ground truth while $\\hat y$ is the predicted value by the model (or hypothesis) $(\\hat y = h(x) = \\theta x)$. We will find $\\theta$ so that the sum of all errors $(\\sum (\\hat y - y))$ becoma as small as possible. This is called loss (or objective) function. To make it more mathematically elegant, let's square the error and average it all (hense the well-know loss function; Mean of Square Errors, MSE).\n",
        "\n",
        "$\\ell (\\theta) = \\frac{1}{m} \\sum_{i=1}^m {(\\hat y - y)}^2$\n",
        "\n",
        "Gradient Descent uses the derivative of $\\ell (\\theta)$ with respect to each $\\theta$ (i.e. gradients) to find the direction of the next move towards the optima. Here it is the Gradient Descent update rule:\n",
        "\n",
        "$\\theta_{new} := \\theta_{old} - \\alpha \\nabla \\ell(\\theta)$ which is the same as:\n",
        "\n",
        "$\\theta_{j\\_new} := \\theta_{j\\_old} - \\alpha x^i_j (\\hat{y}^i - y^i)$ which is the same as:\n",
        "\n",
        "$\\theta_{j\\_new} := \\theta_{j\\_old} - \\alpha x^i_j(\\theta x - y^i)$\n",
        "\n",
        "Knowing all of this, let's implement the gradient descent function:\n"
      ],
      "metadata": {
        "id": "svQtJIFNFFtK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent(X, y, learning_rate, n_iterations):\n",
        "    m = len(y)\n",
        "    theta = np.random.randn(1, 1)  # Initially assign random values to theta\n",
        "    loss_history = [] # save the loss history to visualize learning curve\n",
        "\n",
        "    for iteration in range(n_iterations):\n",
        "        gradients = 2/m * X.T.dot(X.dot(theta) - y)\n",
        "        theta -= learning_rate * gradients # parameters updated\n",
        "        loss = np.mean((X.dot(theta) - y)**2) # compute loss with new parameters\n",
        "        loss_history.append(loss)\n",
        "\n",
        "    return theta, loss_history"
      ],
      "metadata": {
        "id": "xHCppNQXk_Bf"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Gradient step is ready. But first, it requires two hyperparameters to run.\n",
        "\n",
        "In contrast to parameters which are tuned on training data, hyperparamerts are tuned on validation data to fine-tune the optimization procedure. Here we have two hyperparameters; learning rate and number of iteration. Learning rate determines how big the jump toward the optima should be taken after the we recognize the direction of the jump. Too big jumps make too large strides which lead to divergence. Too small jump makes it too long to reach the optima.\n",
        "\n",
        "The second hyper parameter is the number of iterations, which dictates how many times the entire training data should be passed to Gradient Descent until it converges.\n",
        "\n",
        "Convergence is the point where the loss does not reduces anymore, which signals the model does not learn anymore, eaither. It can be checked by computing the difference between previous and current loss in each iteratin.\n",
        "\n",
        "Let's implement these concepts before running the optimization step:"
      ],
      "metadata": {
        "id": "tmYEA8yFPKEs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.1\n",
        "n_iterations = 20"
      ],
      "metadata": {
        "id": "JpPCF3rglBsT"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now everything is in place. Let's run the code and see how it works by visualzing its loss curve."
      ],
      "metadata": {
        "id": "foC8rg4RRlcY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "theta_best, loss_history = gradient_descent(X, y, learning_rate, n_iterations)\n",
        "\n",
        "plt.plot(range(n_iterations), loss_history)\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Cost')\n",
        "plt.title('Gradient Descent Convergence')\n",
        "plt.show()\n",
        "#\n",
        "print(\"Theta found by Gradient Descent:\", theta_best.ravel())"
      ],
      "metadata": {
        "id": "FbIL5o1-Rdqr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optional Excercise 1: Compare the development and training loss and argue if this is over-fit or under-fit\n",
        "\n",
        "Optional Excercise 2: Scale the features and run again. Notice any difference?\n",
        "\n",
        "Optional Excercise 3: Assess the impact of too small/big learing rate along number of iteration\n",
        "\n",
        "Optional Excercise 4: Assess the impact of other type of initialization (all ones, all zeros, random number with different SD) on learning curve  "
      ],
      "metadata": {
        "id": "H-xaLaG9dxji"
      }
    }
  ]
}