{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMAS0/RVmNfbTI8rFPamRFx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AhmadAghaebrahimian/Optimization/blob/main/Newton/NM_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this exercise, we minimize a simple function $f(x) = 2x - log(x)$ employing Newton's method.\n",
        "\n",
        "Let's first visualize it to inspect the minimum point visually.\n"
      ],
      "metadata": {
        "id": "juFFabt1LtcC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fDHnVCW6imq2"
      },
      "outputs": [],
      "source": [
        "# importing required libraries\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_axis = np.linspace(0.01, 1.2, 100) # generate x vector\n",
        "y_axis = 2*x_axis - np.log(x_axis) # compuate y vector\n",
        "\n",
        "plt.plot(x_axis, y_axis, '-r', label='y = 2*x - log(x)')\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OGdtMsy2MNu0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's import some libraries, including a symbolic mathematical library which makes the concept easier to grasp."
      ],
      "metadata": {
        "id": "APlwAvL_Mjbb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sympy import diff, log # symbolic functions"
      ],
      "metadata": {
        "id": "H0kDII5xMxDQ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To find the minimum point of this function, we use the update rule of Newton's method:\n",
        "\n",
        "$x_{new} := x_{old} - \\frac{f'(\\theta_{old})}{f''(\\theta_{old})}$\n",
        "\n",
        "Following the steps described in the workbook, we first the initial minimum:\n",
        "\n",
        "$x_{old} = 0.1$\n",
        "\n",
        "Now, let's define the first and second derivations:\n",
        "\n",
        "$f(x) = 2x - log(x)$\n",
        "\n",
        "$f'(x) = 2 - \\frac{1}{x}$\n",
        "\n",
        "$f(x) = \\frac{1}{x^2}$\n",
        "\n",
        "Using the update rule and the derivations, we can now iterate to find the minimum."
      ],
      "metadata": {
        "id": "GjkTvoE9OF9K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_iter = 10         # Number of iteration\n",
        "x_old = 0.1           # initial optimum point\n",
        "\n",
        "x_list = list()\n",
        "fx_list = list()\n",
        "\n",
        "for _ in range(max_iter):\n",
        "    x_new = x_old - (2 - 1 / x_old) / x_old ** (-2)\n",
        "    fx = 2*x_new - np.log(x_new)\n",
        "    x_list.append(x_new)\n",
        "    fx_list.append(fx)\n",
        "    x_old = x_new\n",
        "\n",
        "print('Optimum/Minimum value: %f:%f'%(x_new, (2*x_new - log(x_new))))"
      ],
      "metadata": {
        "id": "okMPDrnkNymu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's visualize the optimum value:"
      ],
      "metadata": {
        "id": "a4lvPDJoTZuG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(x_axis, y_axis, '-r', label='y = 2*x - log(x)')\n",
        "plt.grid()\n",
        "\n",
        "for x, fx in zip(x_list, fx_list):\n",
        "    plt.plot(x, fx, marker=\"o\", markersize=10, markeredgecolor=\"red\", markerfacecolor=\"green\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YA-HoQ5yTee8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we computed the optimum value, let's use the symbolic library to sanity-check the optimum."
      ],
      "metadata": {
        "id": "6MsCWWbQUteI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sympy.abc import x # symbolic variable x\n",
        "from sympy.calculus.util import Interval, minimum\n",
        "\n",
        "f = 2 * x - log(x)    # Symbolic function definition\n",
        "df = diff(f)          # First derivation\n",
        "ddf = diff(df)        # Second derivation\n",
        "print('First Derivative: ', df)\n",
        "print('Second Derivative: ',ddf)\n",
        "\n",
        "print('Min of f between 0 and 1: ', minimum(2*x - log(x), x, Interval(0, 1)))\n",
        "#print(np.log(2) + 1)"
      ],
      "metadata": {
        "id": "k-7iGhITOFl3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optional exercise 1: Define convergence criteria in the loop to check if the model is converged."
      ],
      "metadata": {
        "id": "ey_l0gCMRmrw"
      }
    }
  ]
}