{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMYLn1wCzANBLVd+9JZvKt0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AhmadAghaebrahimian/Optimization/blob/main/Newton/NM_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the last practice, we used Newton's method to solve an optimization problem for a function with only one variable. Now, let's consider an optimization problem for functions with a multidimensioan variable.\n",
        "\n",
        "In this practise we use Newton method in Logistic Regression which is a very useful algorithm in Machine Learingin. Logistic Regression is a method for classification problems to output discrete values. For example, given an input vector of a tumour with its various measurements, the logistic function classifies it as malignant or benign. Logistic regression has an optimization procedure at its core for which in this excercise, we use Newton method.\n",
        "\n",
        "Let's begin with importing dependencies\n"
      ],
      "metadata": {
        "id": "m0YgVo4Cw7AR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "vhshBUHmhDet"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now it time to lead the data. We use the same dataset that we used for Gradient Descent. As you recall, the Tumor dataset consists of 30 different measurements (features) of 569 moles (instances). Each instance is assigned to one label, either Malignant or Benign as the type of the tumor.\n",
        "\n",
        "Let's load the dataset."
      ],
      "metadata": {
        "id": "6b8jfnteo2vQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "link = 'https://drive.google.com/uc?export=download&id=1m1s6Q7xQfdWf642OqMkzu2nmvoJ0xK_5'\n",
        "data = pd.read_csv(link)\n",
        "# data = data.sample(frac=1).reset_index(drop=True)\n",
        "d = {'M': 1, 'B': 0} # we map labels from string to integer\n",
        "data['diagnosis'] = data['diagnosis'].map(d)\n",
        "print(data.head())"
      ],
      "metadata": {
        "id": "jJ2DUvUIo260"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As before, let's seperate the Tumor data into standard datasets; 80% for training, 10% for development, and 10% for testing."
      ],
      "metadata": {
        "id": "F08AkRIH9MBF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_idx = int(data.shape[0]*0.9)\n",
        "val_idx = int(data.shape[0]*0.8)\n",
        "\n",
        "test_data = data[test_idx:]\n",
        "val_data = data[val_idx:test_idx]\n",
        "data = data[:val_idx]\n",
        "\n",
        "train_Y, train_X = data['diagnosis'], data.drop('diagnosis', axis=1)\n",
        "val_Y, val_X = val_data['diagnosis'], val_data.drop('diagnosis', axis=1)\n",
        "test_Y, test_X = test_data['diagnosis'], test_data.drop('diagnosis', axis=1)\n",
        "\n",
        "print('Training data shape: ', train_X.shape)\n",
        "print('Validation data shape: ', val_X.shape)\n",
        "print('Testing data shape: ', test_X.shape)"
      ],
      "metadata": {
        "id": "51X5jUopqSpE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Also we need the Sigmoid function.\n",
        "\n",
        "$s(x) = \\frac{1}{1+e^{-f(x)}}$\n",
        "\n",
        "Let's implement it."
      ],
      "metadata": {
        "id": "TK3X0hY4qVEs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(x):\n",
        "    return 1/(1+np.exp(-x))"
      ],
      "metadata": {
        "id": "08C0Vnk-qah7"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As before, we also need to implement a function to compute accuracy:"
      ],
      "metadata": {
        "id": "3ueshiqtqpL8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_accuracy(X, y, theta):\n",
        "    y_hat = np.array(sigmoid(X.dot(theta)))\n",
        "\n",
        "    ## Converting y_hat probabilities to prediction, >.5 = 1(M), <.5 = 0(B)\n",
        "    predictions = np.greater(y_hat, 0.5 * np.ones((y_hat.shape[1], 1)))\n",
        "    accuracy = np.count_nonzero(np.equal(predictions, y)) / predictions.shape[0] * 100\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "8u6LPUB8qqTc"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now it is time to implement Newton method. The generalization of Newtonâ€™s method to multidimensional data (also called the Newton-Raphson method) is given by the following update rule using the Hessian matrix:\n",
        "\n",
        "$\\theta_{new} := \\theta_{old} - H^{-1} \\nabla_\\theta \\ell(\\theta)$\n",
        "\n",
        "where the Hessian matrix $H(.)$ is represented as:\n",
        "\n",
        "$H_{ij} = \\frac{\\partial^2 \\ell(\\theta)}{\\partial\\theta_i\\partial\\theta_j}$\n",
        "\n",
        "For Logistic Regression, Hessina Matrix is calculated by:\n",
        "\n",
        "$H = - X^{\\top} \\omega X$\n",
        "\n",
        "and the gradient by:\n",
        "\n",
        "$\\nabla_\\theta \\ell(\\theta) = X^{\\top} (\\hat{y} - y)$\n",
        "\n",
        "where:\n",
        "\n",
        "$\\omega := diag (\\hat{y}(1-\\hat{y}))$\n",
        "\n",
        "Let's implement these steps in one step Newton method:\n",
        "\n"
      ],
      "metadata": {
        "id": "5RHFXNkiqcr-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def newton_step(theta_old, y, X):\n",
        "    y_hat = np.array(sigmoid(X.dot(theta_old[:, 0])), ndmin=2).T  # probability matrix - N x 1\n",
        "    omega = np.diag((y_hat * (1 - y_hat))[:, 0])  # N by N diagonal matrix\n",
        "    hessian = X.T.dot(omega).dot(X)  # 30 by 30 matrix\n",
        "    gradian = X.T.dot(y - y_hat)  # 30 by 1 matrix\n",
        "\n",
        "    # Deal with non-invertible hessian\n",
        "    step = np.dot(np.linalg.inv(hessian + 0.1*np.eye(theta_old.shape[0])), gradian)\n",
        "\n",
        "    theta_new = theta_old + step\n",
        "    return theta_new"
      ],
      "metadata": {
        "id": "rZAPq6h8qhba"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's define a function to check if the model is converged or not. It is done by defining a treshold and check if it superceeds the difference between $\\theta_{old}$ and $\\theta_{new}$."
      ],
      "metadata": {
        "id": "c_tiQDOtw5-s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_converged(theta_old, theta_new, threshold, iters):\n",
        "    theta_delta = np.abs(theta_old - theta_new)\n",
        "    return not (np.any(theta_delta > threshold) and iters < max_iters)"
      ],
      "metadata": {
        "id": "vF5BIXl3xCNE"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameters"
      ],
      "metadata": {
        "id": "dAWEWlSCqs0l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_iters = 10\n",
        "threshold = 0.01 # convergence tolerance"
      ],
      "metadata": {
        "id": "SSCXV4WTqv4H"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, everthing is in place. Let's run everything together and visualize the accuracy curve."
      ],
      "metadata": {
        "id": "7u3ZUa34qzVi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "theta_old, theta_new = np.ones((30, 1)), np.zeros((30, 1))\n",
        "iter_count = 0\n",
        "converged = False\n",
        "\n",
        "while not converged:\n",
        "    print('Validation Accuracy {}% at Iteration {}'.format(get_accuracy(val_X, val_Y.to_frame(), theta_old), iter_count))\n",
        "    theta_old = theta_new\n",
        "    theta_new = newton_step(theta_new, train_Y.to_frame(), train_X)\n",
        "    iter_count += 1\n",
        "    converged = get_converged(theta_old, theta_new, threshold, iter_count)\n",
        "\n",
        "print('Test Accuracy {}% at Iteration {}'.format(get_accuracy(val_X, val_Y.to_frame(), theta_new), iter_count))"
      ],
      "metadata": {
        "id": "KCBZ7Q8pqyKO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optional Excercise 1: Add a bias terms to $h(x)$ and make the required changes.\n",
        "\n",
        "Optional Excercise 2: Scale the features and run again. Notice any difference?\n",
        "\n",
        "Optional Excercise 3: One way to check if the optimization work is to check if the grdients is constatntly decresing or not. Plot the gradient cureve to investigate this."
      ],
      "metadata": {
        "id": "BrTuiOMjerp2"
      }
    }
  ]
}