{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN6N9NU65YCpakEBhEykP3X",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AhmadAghaebrahimian/Optimization/blob/main/Newton/NM_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the last practice, we used Newton's method to solve an optimization problem for a function with only one variable. Now, let's consider an optimization problem for functions with a multidimensional variable.\n",
        "\n",
        "In this exercise, we use Newton's method of Logistic Regression which is a very useful algorithm in Machine Learning. Logistic Regression is a method for classification problems to output discrete values. For example, given an input vector of a tumor with its various measurements, the logistic function classifies it as malignant or benign. Logistic regression has an optimization procedure at its core for which in this exercise, we use the Newton method.\n",
        "\n",
        "Let's begin with importing dependencies"
      ],
      "metadata": {
        "id": "m0YgVo4Cw7AR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vhshBUHmhDet"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now it is time to lead the data. We use the same dataset that we used for Gradient Descent. As you recall, the Tumor dataset consists of 30 different measurements (features) of 569 moles (instances). Each instance is assigned to one label, either Malignant or Benign as the type of the tumor.\n",
        "\n",
        "Let's load the dataset."
      ],
      "metadata": {
        "id": "6b8jfnteo2vQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "link = 'https://drive.google.com/uc?export=download&id=1m1s6Q7xQfdWf642OqMkzu2nmvoJ0xK_5'\n",
        "data = pd.read_csv(link)\n",
        "# data = data.sample(frac=1).reset_index(drop=True)\n",
        "d = {'M': 1, 'B': 0} # we map labels from string to integer\n",
        "data['diagnosis'] = data['diagnosis'].map(d)\n",
        "print(data.head())"
      ],
      "metadata": {
        "id": "jJ2DUvUIo260"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As before, let's separate the Tumor data into standard datasets; 80% for training, 10% for development, and 10% for testing."
      ],
      "metadata": {
        "id": "F08AkRIH9MBF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_idx = int(data.shape[0]*0.9)\n",
        "val_idx = int(data.shape[0]*0.8)\n",
        "\n",
        "test_data = data[test_idx:]\n",
        "val_data = data[val_idx:test_idx]\n",
        "data = data[:val_idx]\n",
        "\n",
        "train_Y, train_X = data['diagnosis'], data.drop('diagnosis', axis=1)\n",
        "val_Y, val_X = val_data['diagnosis'], val_data.drop('diagnosis', axis=1)\n",
        "test_Y, test_X = test_data['diagnosis'], test_data.drop('diagnosis', axis=1)\n",
        "\n",
        "print('Training data shape: ', train_X.shape)\n",
        "print('Validation data shape: ', val_X.shape)\n",
        "print('Testing data shape: ', test_X.shape)"
      ],
      "metadata": {
        "id": "51X5jUopqSpE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Also, we need the Sigmoid function.\n",
        "\n",
        "$s(x) = \\frac{1}{1+e^{-f(x)}}$\n",
        "\n",
        "Let's implement it."
      ],
      "metadata": {
        "id": "TK3X0hY4qVEs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(x):\n",
        "    return 1/(1+np.exp(-x))"
      ],
      "metadata": {
        "id": "08C0Vnk-qah7"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As before, we also need to implement a function to compute accuracy:"
      ],
      "metadata": {
        "id": "3ueshiqtqpL8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_accuracy(X, y, theta):\n",
        "    y_hat = np.array(sigmoid(X.dot(theta)))\n",
        "\n",
        "    ## Converting y_hat probabilities to prediction, >.5 = 1(M), <.5 = 0(B)\n",
        "    predictions = np.greater(y_hat, 0.5 * np.ones((y_hat.shape[1], 1)))\n",
        "    accuracy = np.count_nonzero(np.equal(predictions, y)) / predictions.shape[0] * 100\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "8u6LPUB8qqTc"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now it is time to implement Newton's method. The generalization of Newtonâ€™s method to multidimensional data (also called the Newton-Raphson method) is given by the following update rule using the Hessian matrix:\n",
        "\n",
        "$\\theta_{new} := \\theta_{old} - H^{-1} \\nabla_\\theta \\ell(\\theta)$\n",
        "\n",
        "where the Hessian matrix $H(.)$ is represented as:\n",
        "\n",
        "$H_{ij} = \\frac{\\partial^2 \\ell(\\theta)}{\\partial\\theta_i\\partial\\theta_j}$\n",
        "\n",
        "For Logistic Regression, Hessian Matrix is calculated by:\n",
        "\n",
        "$H = - X^{\\top} \\omega X$\n",
        "\n",
        "and the gradient by:\n",
        "\n",
        "$\\nabla_\\theta \\ell(\\theta) = X^{\\top} (\\hat{y} - y)$\n",
        "\n",
        "where:\n",
        "\n",
        "$\\omega := diag (\\hat{y}(1-\\hat{y}))$\n",
        "\n",
        "Let's implement these steps for one step Newton method:\n",
        "\n"
      ],
      "metadata": {
        "id": "5RHFXNkiqcr-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def newton_step(theta_old, y, X):\n",
        "    y_hat = np.array(sigmoid(X.dot(theta_old[:, 0])), ndmin=2).T  # probability matrix - N x 1\n",
        "    omega = np.diag((y_hat * (1 - y_hat))[:, 0])  # N by N diagonal matrix\n",
        "    hessian = X.T.dot(omega).dot(X)  # 30 by 30 matrix\n",
        "    gradian = X.T.dot(y - y_hat)  # 30 by 1 matrix\n",
        "\n",
        "    # Deal with non-invertible hessian\n",
        "    step = np.dot(np.linalg.inv(hessian + 0.1*np.eye(theta_old.shape[0])), gradian)\n",
        "\n",
        "    theta_new = theta_old + step\n",
        "    return theta_new"
      ],
      "metadata": {
        "id": "rZAPq6h8qhba"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's define a function to check if the model is converged or not. It is done by defining a threshold and checking if it supersedes the difference between $\\theta_{old}$ and $\\theta_{new}$."
      ],
      "metadata": {
        "id": "c_tiQDOtw5-s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_converged(theta_old, theta_new, threshold, iters):\n",
        "    theta_delta = np.abs(theta_old - theta_new)\n",
        "    return not (np.any(theta_delta > threshold) and iters < max_iters)"
      ],
      "metadata": {
        "id": "vF5BIXl3xCNE"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameters"
      ],
      "metadata": {
        "id": "dAWEWlSCqs0l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_iters = 10\n",
        "threshold = 0.01 # convergence tolerance"
      ],
      "metadata": {
        "id": "SSCXV4WTqv4H"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, everything is in place. Let's run everything together."
      ],
      "metadata": {
        "id": "7u3ZUa34qzVi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "theta_old, theta_new = np.ones((30, 1)), np.zeros((30, 1))\n",
        "iter_count = 0\n",
        "converged = False\n",
        "\n",
        "while not converged:\n",
        "    print('Validation Accuracy {}% at Iteration {}'.format(get_accuracy(val_X, val_Y.to_frame(), theta_old), iter_count))\n",
        "    theta_old = theta_new\n",
        "    theta_new = newton_step(theta_new, train_Y.to_frame(), train_X)\n",
        "    iter_count += 1\n",
        "    converged = get_converged(theta_old, theta_new, threshold, iter_count)\n",
        "\n",
        "print('Test Accuracy {}% at Iteration {}'.format(get_accuracy(val_X, val_Y.to_frame(), theta_new), iter_count))"
      ],
      "metadata": {
        "id": "KCBZ7Q8pqyKO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "404da834-280e-4a46-b947-db06ffbc5b99"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy 21.052631578947366% at Iteration 0\n",
            "Validation Accuracy 78.94736842105263% at Iteration 1\n",
            "Validation Accuracy 98.24561403508771% at Iteration 2\n",
            "Validation Accuracy 100.0% at Iteration 3\n",
            "Validation Accuracy 96.49122807017544% at Iteration 4\n",
            "Validation Accuracy 94.73684210526315% at Iteration 5\n",
            "Validation Accuracy 92.98245614035088% at Iteration 6\n",
            "Validation Accuracy 92.98245614035088% at Iteration 7\n",
            "Validation Accuracy 92.98245614035088% at Iteration 8\n",
            "Validation Accuracy 92.98245614035088% at Iteration 9\n",
            "Test Accuracy 92.98245614035088% at Iteration 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optional Exercise 1: One way to check if the optimization works is to check if the gradient is constantly decreasing or not. Plot the gradient curve to investigate this.\n",
        "\n",
        "Optional Exercise 2: Compare the results on the Tumor dataset using the Gradient Descent and Newton method regarding performance and throughput."
      ],
      "metadata": {
        "id": "BrTuiOMjerp2"
      }
    }
  ]
}